<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">

<!--Converted with LaTeX2HTML 2002-2-1 (1.71)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<HTML>
<HEAD>
<TITLE>Performance and scalability</TITLE>
<META NAME="description" CONTENT="Performance and scalability">
<META NAME="keywords" CONTENT="DocMain">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">

<META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=iso-8859-1">
<META NAME="Generator" CONTENT="LaTeX2HTML v2002-2-1">
<META HTTP-EQUIV="Content-Style-Type" CONTENT="text/css">

<LINK REL="STYLESHEET" HREF="DocMain.css">

<LINK REL="next" HREF="node9.html">
<LINK REL="previous" HREF="node7.html">
<LINK REL="up" HREF="node2.html">
<LINK REL="next" HREF="node9.html">
</HEAD>

<BODY >

<DIV CLASS="navigation"><!--Navigation Panel-->
<A NAME="tex2html478"
  HREF="node9.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next" SRC="next.png"></A> 
<A NAME="tex2html474"
  HREF="node2.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up" SRC="up.png"></A> 
<A NAME="tex2html468"
  HREF="node7.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous" SRC="prev.png"></A> 
<A NAME="tex2html476"
  HREF="node1.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents" SRC="contents.png"></A>  
<BR>
<B> Next:</B> <A NAME="tex2html479"
  HREF="node9.html">System components</A>
<B> Up:</B> <A NAME="tex2html475"
  HREF="node2.html">Overview</A>
<B> Previous:</B> <A NAME="tex2html469"
  HREF="node7.html">Evaluation of automated subject</A>
 &nbsp; <B>  <A NAME="tex2html477"
  HREF="node1.html">Contents</A></B> 
<BR>
<BR></DIV>
<!--End of Navigation Panel-->
<!--Table of Child-Links-->
<A NAME="CHILD_LINKS"><STRONG>Subsections</STRONG></A>

<UL CLASS="ChildLinks">
<LI><A NAME="tex2html480"
  HREF="node8.html#SECTION02061000000000000000">Speed</A>
<LI><A NAME="tex2html481"
  HREF="node8.html#SECTION02062000000000000000">Space</A>
<LI><A NAME="tex2html482"
  HREF="node8.html#SECTION02063000000000000000">Crawling strategy</A>
</UL>
<!--End of Table of Child-Links-->
<HR>

<H1><A NAME="SECTION02060000000000000000"></A>
<A NAME="performance"></A>
<BR>
Performance and scalability
</H1>

<P>
Performance evaluation of the automated subject classification component is
treated in section <A HREF="node7.html#autoclasseval">5</A>.

<P>

<H2><A NAME="SECTION02061000000000000000">
Speed</A>
</H2>

<P>
Performance in terms of number of URLs treated per minute is of course
highly dependent on a number of circumstances like network load,
capacity of the machine, the selection of URLs to crawl, configuration
details, number of crawlers used, etc.
In general, within rather wide limits, you could expect the Combine
system to handle up to 200 URLs per
minute. By ``handle'' we mean everything from scheduling of URLs, fetching
 pages over the network, parsing the page,
automated subject classification, recycling of new links, to storing the structured record in
a relational database. This holds for small simple crawls starting
from scratch to large complicated topic specific crawls with millions
of records.

<P>

<DIV ALIGN="CENTER"><A NAME="crawlspeed"></A><A NAME="9"></A>
<TABLE>
<CAPTION ALIGN="BOTTOM"><STRONG>Figure 4:</STRONG>
Combine crawler performance, using no focus and configuration optimized for speed.</CAPTION>
<TR><TD>
<DIV ALIGN="CENTER">
<IMG
 WIDTH="624" HEIGHT="445" ALIGN="BOTTOM" BORDER="0"
 SRC="img22.png"
 ALT="\includegraphics[height=0.4\textheight]{CrawlerSpeed.ps}">

</DIV></TD></TR>
</TABLE>
</DIV>

<P>
The prime way of increasing performance is to use more than one
crawler for a job. This is handled by the <code></code> switch
used together with the <TT>combineCtrl start</TT> command for example
<code></code>
will start 5 crawlers working together on the job 'MyCrawl'. The
effect of using more than one crawler on crawling speed is illustrated
in figure <A HREF="#crawlspeed">4</A> and the resulting speedup is shown in table <A HREF="#speedup">1</A>.

<P>
<BR><P></P>
<DIV ALIGN="CENTER"><A NAME="23"></A>
<TABLE>
<CAPTION><STRONG>Table 1:</STRONG>
Speedup of crawling vs number of crawlers</CAPTION>
<TR><TD>
<DIV ALIGN="CENTER">
<TABLE CELLPADDING=3 BORDER="1" ALIGN="CENTER">
<TR><TD ALIGN="LEFT"><B>No of crawlers</B></TD>
<TD ALIGN="CENTER">1</TD>
<TD ALIGN="CENTER">2</TD>
<TD ALIGN="CENTER">5</TD>
<TD ALIGN="CENTER">10</TD>
<TD ALIGN="CENTER">15</TD>
<TD ALIGN="CENTER">20</TD>
</TR>
<TR><TD ALIGN="LEFT"><B>Speedup</B></TD>
<TD ALIGN="CENTER">1</TD>
<TD ALIGN="CENTER">2.0</TD>
<TD ALIGN="CENTER">4.8</TD>
<TD ALIGN="CENTER">8.2</TD>
<TD ALIGN="CENTER">9.8</TD>
<TD ALIGN="CENTER">11.0</TD>
</TR>
</TABLE>
</DIV>

<A NAME="speedup"></A></TD></TR>
</TABLE>
</DIV><P></P>
<BR>

<P>
Configuration also has an effect on performance. In Figure
<A HREF="#config">5</A> performance improvements based on configuration changes are shown. The choice of
algorithm for automated classification turns out to have biggest
influence on performance, where <A HREF="node6.html#pos">algorithm 2</A> (<TT>classifyPlugIn = Combine::PosCheck_record</TT> - Pos in Figure <A HREF="#config">5</A>) is much
faster than <A HREF="node6.html#std">algorithm 1</A> (<TT>classifyPlugIn = Combine::Check_record</TT> - Std in Figure <A HREF="#config">5</A>).
Configuration optimization consisted of not using
Tidy to clean HTML (<TT>useTidy = 0</TT>) and not storing the original
page in the database (<TT>saveHTML = 0</TT>).
 Tweaking of other configuration variables (like disabling logging
to the MySQL database <TT>Loglev = 0</TT>) also has an effect
on performance but to a lesser degree.

<P>

<DIV ALIGN="CENTER"><A NAME="config"></A><A NAME="46"></A>
<TABLE>
<CAPTION ALIGN="BOTTOM"><STRONG>Figure 5:</STRONG>
Effect of configuration changes on focused crawler performance, using 10 crawlers and a topic definition with 2512 terms.</CAPTION>
<TR><TD>
<DIV ALIGN="CENTER">
<IMG
 WIDTH="621" HEIGHT="445" ALIGN="BOTTOM" BORDER="0"
 SRC="img23.png"
 ALT="\includegraphics[height=0.4\textheight]{Config.ps}">

</DIV></TD></TR>
</TABLE>
</DIV>

<P>

<H2><A NAME="SECTION02062000000000000000">
Space</A>
</H2>

<P>
Storing structured records including the original document takes
quite a lot of disk space. On average 25 kB per record is
used by MySQL. This includes the administrative overhead needed
for the operation of the crawler. A database with 100&nbsp;000 records
needs at least 2.5 GB on disk. Deciding not to store the original
page in the database (<TT>saveHTML = 0</TT>) gives considerable space
savings. On average 8 kB per is used without the original HTML.

<P>
Exporting records in the ALVIS XML format further increases size
to 42 kB per record. Using the slight less redundant XML-format
<TT>combine</TT> uses 27 kB per record. Thus 100&nbsp;000 records will generate
a file of size 3 to 4 GB. The really compact Dublin Core format (<TT>dc</TT>) generates 0.65 kB per record.

<P>

<H2><A NAME="SECTION02063000000000000000">
Crawling strategy</A>
</H2>

<P>
In [<A
 HREF="node10.html#Rafael06">19</A>] four different crawling strategies are studied:
<DL>
<DT><STRONG>BreadthFirst</STRONG></DT>
<DD>The simplest strategy for crawling. 
It does not utilize heuristics in deciding which URL to visit next. 
It uses the frontier as a FIFO queue, crawling links in the order in which 
they are encountered.

<P>
</DD>
<DT><STRONG>BestFirst</STRONG></DT>
<DD>The basic idea is that given a frontier of URLs, the best URL
according to some estimation criterion is selected for crawling,
using the frontier as a priority queue.
In this implementation, the URL selection process is guided by 
the topic score of the source page as calculated by Combine.

<P>
</DD>
<DT><STRONG>PageRank</STRONG></DT>
<DD>The same as Best-First but ordered by PageRank calculated
from the pages crawled so far.

<P>
</DD>
<DT><STRONG>BreadthFirstTime</STRONG></DT>
<DD>A version of BreadthFirst.
It is based on the idea of not accessing the same server during 
a certain period of time in order not to overload servers.
Thus, a page is fetched if and only if 
a certain time threshold is exceeded 
since the last access to the server of that page.

<P>
</DD>
</DL>

<P>
Results from a simulated crawl (figure <A HREF="#crawlstrategy">6</A> from [<A
 HREF="node10.html#Rafael06">19</A>]) show that
at first PageRank performs best but BreadthFirstTime (which is used in Combine) prevails
in the long run, although differences are small.

<P>

<DIV ALIGN="CENTER"><A NAME="crawlstrategy"></A><A NAME="62"></A>
<TABLE>
<CAPTION ALIGN="BOTTOM"><STRONG>Figure 6:</STRONG>
Total number of relevant pages visited</CAPTION>
<TR><TD>
<DIV ALIGN="CENTER">
<IMG
 WIDTH="727" HEIGHT="882" ALIGN="BOTTOM" BORDER="0"
 SRC="img24.png"
 ALT="\includegraphics[width=140mm height=90mm]{crawl.ps}">
  
</DIV></TD></TR>
</TABLE>
</DIV>


<DIV CLASS="navigation"><HR>
<!--Navigation Panel-->
<A NAME="tex2html478"
  HREF="node9.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next" SRC="next.png"></A> 
<A NAME="tex2html474"
  HREF="node2.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up" SRC="up.png"></A> 
<A NAME="tex2html468"
  HREF="node7.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous" SRC="prev.png"></A> 
<A NAME="tex2html476"
  HREF="node1.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents" SRC="contents.png"></A>  
<BR>
<B> Next:</B> <A NAME="tex2html479"
  HREF="node9.html">System components</A>
<B> Up:</B> <A NAME="tex2html475"
  HREF="node2.html">Overview</A>
<B> Previous:</B> <A NAME="tex2html469"
  HREF="node7.html">Evaluation of automated subject</A>
 &nbsp; <B>  <A NAME="tex2html477"
  HREF="node1.html">Contents</A></B> </DIV>
<!--End of Navigation Panel-->
<ADDRESS>
root
2008-10-02
</ADDRESS>
</BODY>
</HTML>
