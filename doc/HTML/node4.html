<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">

<!--Converted with LaTeX2HTML 2002-2-1 (1.70)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<HTML>
<HEAD>
<TITLE>Open Source distribution, Installation</TITLE>
<META NAME="description" CONTENT="Open Source distribution, Installation">
<META NAME="keywords" CONTENT="DocMain">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">

<META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=iso-8859-1">
<META NAME="Generator" CONTENT="LaTeX2HTML v2002-2-1">
<META HTTP-EQUIV="Content-Style-Type" CONTENT="text/css">

<LINK REL="STYLESHEET" HREF="DocMain.css">

<LINK REL="next" HREF="node5.html">
<LINK REL="previous" HREF="node3.html">
<LINK REL="up" HREF="node2.html">
<LINK REL="next" HREF="node5.html">
</HEAD>

<BODY >

<DIV CLASS="navigation"><!--Navigation Panel-->
<A NAME="tex2html280"
  HREF="node5.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next" SRC="next.png"></A> 
<A NAME="tex2html276"
  HREF="node2.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up" SRC="up.png"></A> 
<A NAME="tex2html270"
  HREF="node3.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous" SRC="prev.png"></A> 
<A NAME="tex2html278"
  HREF="node1.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents" SRC="contents.png"></A>  
<BR>
<B> Next:</B> <A NAME="tex2html281"
  HREF="node5.html">Configuration</A>
<B> Up:</B> <A NAME="tex2html277"
  HREF="node2.html">Overview</A>
<B> Previous:</B> <A NAME="tex2html271"
  HREF="node3.html">Introduction</A>
 &nbsp; <B>  <A NAME="tex2html279"
  HREF="node1.html">Contents</A></B> 
<BR>
<BR></DIV>
<!--End of Navigation Panel-->
<!--Table of Child-Links-->
<A NAME="CHILD_LINKS"><STRONG>Subsections</STRONG></A>

<UL CLASS="ChildLinks">
<LI><A NAME="tex2html282"
  HREF="node4.html#SECTION02021000000000000000">Installation</A>
<UL>
<LI><A NAME="tex2html283"
  HREF="node4.html#SECTION02021100000000000000">Installation from source for the impatient</A>
<LI><A NAME="tex2html284"
  HREF="node4.html#SECTION02021200000000000000">Porting to not supported operating systems - dependencies</A>
<LI><A NAME="tex2html285"
  HREF="node4.html#SECTION02021300000000000000">Automated Debian/Ubuntu installation</A>
<LI><A NAME="tex2html286"
  HREF="node4.html#SECTION02021400000000000000">Manual installation</A>
<LI><A NAME="tex2html287"
  HREF="node4.html#SECTION02021500000000000000">Installation test</A>
</UL>
<BR>
<LI><A NAME="tex2html288"
  HREF="node4.html#SECTION02022000000000000000">Getting started</A>
<LI><A NAME="tex2html289"
  HREF="node4.html#SECTION02023000000000000000">Detailed documentation</A>
<LI><A NAME="tex2html290"
  HREF="node4.html#SECTION02024000000000000000">Use scenarios</A>
<UL>
<LI><A NAME="tex2html291"
  HREF="node4.html#SECTION02024100000000000000">General crawling without restrictions</A>
<LI><A NAME="tex2html292"
  HREF="node4.html#SECTION02024200000000000000">Focused crawling - domain restrictions</A>
<LI><A NAME="tex2html293"
  HREF="node4.html#SECTION02024300000000000000">Focused crawling - topic specific</A>
</UL></UL>
<!--End of Table of Child-Links-->
<HR>

<H1><A NAME="SECTION02020000000000000000"></A>
<A NAME="distr"></A>
<BR>
Open Source distribution, Installation
</H1>
The focused crawler have been restructured and packaged as a Debian
package in order to ease distribution and installation. The package
contains dependency information to make sure that all software that is
needed to run the crawler is installed at the same time. In connection
with this we have also packaged a number of necessary Perl-modules as
Debian packages.

<P>
All software and packages are available from the <A NAME="tex2html5"
  HREF="http://combine.it.lth.se/">Combine focused crawler Web-site</A>.

<P>

<H2><A NAME="SECTION02021000000000000000">
Installation</A>
</H2>
This distribution is developed and tested on Linux systems.
It is implemented entirely in Perl and uses the <A NAME="tex2html6"
  HREF="http://www.mysql.com/">MySQL</A>
database system, both of which are supported on many other
operating systems. Porting to other UNIX dialects should be easy.

<P>
The system is distributed either as source or as a Debian package.

<P>

<H3><A NAME="SECTION02021100000000000000">
Installation from source for the impatient</A>
</H3>
Unless you are on a system supporting Debian packages (in which case look at <A HREF="#debian">Automated installation</A>) you should
download and unpack the source.
The following command sequence will then install Combine:
<PRE>
perl Makefile.PL
make
make test
make install
mkdir /etc/combine
cp conf/* /etc/combine/
mkdir /var/run/combine
</PRE>

<P>
Test that it all works (run as root)
<BR><TT>./doc/InstallationTest.pl</TT>

<P>

<H3><A NAME="SECTION02021200000000000000">
Porting to not supported operating systems - dependencies</A>
</H3>
In order to port the system to another platform, you
have to verify the availability, for this platform, of the two main systems:

<UL>
<LI><A NAME="tex2html7"
  HREF="http://www.cpan.org/ports/index.html">Perl</A>
</LI>
<LI><A NAME="tex2html8"
  HREF="http://dev.mysql.com/downloads/">MySQL version <SPAN CLASS="MATH"><IMG
 WIDTH="18" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img2.png"
 ALT="$\geq$"></SPAN> 4.1</A>
</LI>
</UL>
If they are supported you stand a good chance to port the system.

<P>
Furthermore
the <A HREF="node14.html#extmods">external Perl modules</A> should be verified to work
on the new platform.

<P>
Perl modules are most easily installed 
using the Perl CPAN automated system
<BR>(<TT>perl -MCPAN -e shell</TT>).

<P>
Optionally these external programs will be used if they are 
installed on your system.

<UL>
<LI>antiword (parsing MSWord files)
</LI>
<LI>detex (parsing TeX files)
</LI>
<LI>pdftohtml (parsing PDF files)
</LI>
<LI>pstotext (parsing PS and PDF files, needs ghostview)
</LI>
<LI>xlhtml (parsing MSExcel files)
</LI>
<LI>ppthtml (parsing PowerPoint files)
</LI>
<LI>unrtf (parsing RTF files)
</LI>
<LI>tth (parsing TeX files)
</LI>
<LI>untex (parsing TeX files)
</LI>
</UL>

<P>

<H3><A NAME="SECTION02021300000000000000"></A>
<A NAME="debian"></A>
<BR>
Automated Debian/Ubuntu installation
</H3>

<UL>
<LI>add the following lines to your /etc/apt/sources.list
<BR><TT>deb http://combine.it.lth.se/ debian/</TT>
</LI>
<LI>give the commands
<BR><TT>apt-get update
<BR>
apt-get install combine</TT>
</LI>
</UL>
This also installs all dependencies such as MySQL and a lot of necessary
Perl modules.

<P>

<H3><A NAME="SECTION02021400000000000000">
Manual installation</A>
</H3>

<P>
<A NAME="tex2html9"
  HREF="http://combine.it.lth.se/#downloads">Download the latest distribution</A>.

<P>
Install all software that Combine depends on (see above).

<P>
Unpack the archive with <TT>tar zxf </TT>
<BR>
This will create a directory named <TT>combine-XX</TT> with
a number of subdirectories including <TT>bin, Combine, doc, and conf</TT>.

<P>
'<TT>bin</TT>' contains the executable programs.

<P>
'<TT>Combine</TT>' contains needed Perl modules. Should be copied to
somewhere Perl will find them, typically <TT>/usr/share/perl5/Combine/</TT>.

<P>
'<TT>conf</TT>' contains the default configuration files. Combine looks for them
in <TT>/etc/combine/</TT> so they need to be copied there.

<P>
'<TT>doc</TT>' contains documentation.

<P>
The following command sequence will install Combine:
<PRE>
perl Makefile.PL
make
make test
make install
mkdir /etc/combine
cp conf/* /etc/combine/
mkdir /var/run/combine
</PRE>

<P>

<H3><A NAME="SECTION02021500000000000000">
Installation test</A>
</H3>
Harvest 1 URL by doing:
<BR><PRE>
sudo combineINIT --jobname aatest --topic /etc/combine/Topic_carnivor.txt 
combine --jobname aatest --harvest http://combine.it.lth.se/CombineTests/InstallationTest.html
combineExport --jobname aatest --profile dc
</PRE>
and verify that the output, except for dates and order, looks like
<BR><PRE>
&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;documentCollection version="1.1" xmlns:dc="http://purl.org/dc/elements/1.1/"&gt;
&lt;metadata xmlns:dc="http://purl.org/dc/elements/1.1/"&gt;
&lt;dc:format&gt;text/html&lt;/dc:format&gt;
&lt;dc:format&gt;text/html; charset=iso-8859-1&lt;/dc:format&gt;
&lt;dc:subject&gt;Carnivorous plants&lt;/dc:subject&gt;
&lt;dc:subject&gt;Drosera&lt;/dc:subject&gt;
&lt;dc:subject&gt;Nepenthes&lt;/dc:subject&gt;
&lt;dc:title transl="yes"&gt;Installation test for Combine&lt;/dc:title&gt;
&lt;dc:description&gt;&lt;/dc:description&gt;
&lt;dc:date&gt;2006-05-19 9:57:03&lt;/dc:date&gt;
&lt;dc:identifier&gt;http://combine.it.lth.se/CombineTests/InstallationTest.html&lt;/dc:identifier&gt;
&lt;dc:language&gt;en&lt;/dc:language&gt;
&lt;/metadata&gt;
</PRE>

<P>
Or run - as root - the script
<A HREF="node15.html#InstTest"><TT>./doc/InstallationTest.pl</TT></A>
which essentially does the same thing.

<P>

<H2><A NAME="SECTION02022000000000000000"></A>
<A NAME="gettingstarted"></A>
<BR>
Getting started
</H2>
A simple example work-flow for a trivial crawl job name 'aatest' might look like:

<P>

<OL>
<LI>Initialize database and configuration (needs root privileges)
<BR><TT>sudo combineINIT <code>--</code>jobname aatest</TT>
</LI>
<LI><A NAME="load"></A> Load some seed URLs like (you can repeat this command with different URLs as many times as you wish)
<BR><TT>echo 'http://combine.it.lth.se/' | combineCtrl  load <code>--</code>jobname aatest</TT>
</LI>
<LI><A NAME="crawl"></A>   Start 2 harvesting processes
<BR><TT>combineCtrl  start <code>--</code>jobname aatest <code>--</code>harvesters 2</TT>

<P>
</LI>
<LI>Let it run for some time. Status and progress can be checked using
the program '<TT>combineCtrl <code>--</code>jobname aatest</TT>'
with various parameters.

<P>
</LI>
<LI>When satisfied kill the crawlers
<BR><TT>combineCtrl kill <code>--</code>jobname aatest</TT>
</LI>
<LI>Export data records in the ALVIS XML format
<BR><TT>combineExport <code>--</code>jobname aatest <code>--</code>profile alvis</TT>

<P>
</LI>
<LI>If you want to schedule a recheck for all the crawled pages stored in the database
<BR><TT>combineCtrl reharvest <code>--</code>jobname aatest</TT>
</LI>
<LI>go back to <A HREF="#crawl">3</A> for continuous operation.
</LI>
</OL>

<P>
Once a job is initialized it is controlled using
<TT>combineCtrl</TT>. Crawled data is exported using <TT>combineExport</TT>.

<P>

<H2><A NAME="SECTION02023000000000000000">
Detailed documentation</A>
</H2>
The latest, updated, detailed documentation is always available
<A NAME="tex2html10"
  HREF="http://combine.it.lth.se/documentation/">online</A>.

<P>

<H2><A NAME="SECTION02024000000000000000">
Use scenarios</A>
</H2>

<H3><A NAME="SECTION02024100000000000000">
General crawling without restrictions</A>
</H3>
Same procedure as in section <A HREF="#gettingstarted">2.2</A>. This way of
crawling is not recommended for the Combine system since it will
generate really huge databases without any focus.

<P>

<H3><A NAME="SECTION02024200000000000000">
Focused crawling - domain restrictions</A>
</H3>
Create a focused database with all pages from a Web-site. In this
use scenario we will crawl the Combine site and the ALVIS site.
The database is to be continuously updated, ie all pages have to be
regularly tested for changes, deleted pages should be removed from
the database, and newly created pages added.

<OL>
<LI>Initialize database and configuration
<BR><TT>sudo combineINIT <code>--</code>jobname focustest</TT>

<P>
</LI>
<LI>Edit the configuration to provide the desired focus
<BR>
Change the <TT>&lt;allow&gt;</TT> part in <TT>/etc/combine/focustest/combine.cfg</TT> from
<BR><PRE>
#use either URL or HOST: (obs ':') to match regular expressions to either the
#full URL or the HOST part of a URL.
&lt;allow&gt;
#Allow crawl of URLs or hostnames that matches these regular expressions
HOST: .*$
&lt;/allow&gt;
</PRE>
to
<BR><PRE>
#use either URL or HOST: (obs ':') to match regular expressions to either the
#full URL or the HOST part of a URL.
&lt;allow&gt;
#Allow crawl of URLs or hostnames that matches these regular expressions
HOST: www\.alvis\.info$
HOST: combine\.it\.lth\.se$
&lt;/allow&gt;
</PRE>
The escaping of '.' by writing '<code>\.</code>' is necessary since the patterns
actually are Perl regular expressions. Similarly the ending '$'
indicates that the host string should end here, so for example
a Web server on <TT>www.alvis.info.com</TT> (if such a one exists) will
not be crawled.

<P>
</LI>
<LI>Load seed URLs
<BR><TT>echo 'http://combine.it.lth.se/' | combineCtrl  load <code>--</code>jobname&nbsp;focustest</TT>
<BR><TT>echo 'http://www.alvis.info/' | combineCtrl  load <code>--</code>jobname focustest</TT>

<P>
</LI>
<LI>Start 1 harvesting process
<BR><TT>combineCtrl  start <code>--</code>jobname focustest</TT>

<P>
</LI>
<LI>Daily export all data records in the ALVIS XML format
<BR><TT>combineExport <code>--</code>jobname focustest <code>--</code>profile alvis</TT>
<BR>
and schedule all pages for re-harvesting
<BR><TT>combineCtrl reharvest <code>--</code>jobname focustest</TT>
</LI>
</OL>

<P>

<H3><A NAME="SECTION02024300000000000000">
Focused crawling - topic specific</A>
</H3>

<P>
Create and maintain a topic specific crawled database for the topic 'Carnivorous plants'.

<P>

<OL>
<LI>Create a topic definition (see section <A HREF="node6.html#topicdef">4.5.1</A>) in a local file named <TT>cpTopic.txt</TT>. (Can be done by copying <TT>/etc/combine/Topic_carnivor.txt</TT> since it happens to be just that.)

<P>
</LI>
<LI>Create a file named <TT>cpSeedURLs.txt</TT> with seed URLs for this
topic, containing the URLs:
<PRE>
http://www.sarracenia.com/faq.html
http://dmoz.org/Home/Gardening/Plants/Carnivorous_Plants/
http://www.omnisterra.com/bot/cp_home.cgi
http://www.vcps.au.com/
http://www.murevarn.se/links.html
</PRE>

<P>
</LI>
<LI>Initialization
<BR><TT>sudo combineINIT <code>--</code>jobname cptest <code>--</code>topic cpTopic.txt</TT>

<P>
This enables topic checking and focused crawl mode by setting
configuration variable <TT>doCheckRecord = 1</TT> and copying a topic definition file (<TT>cpTopic.txt</TT>) to
<BR><TT>/etc/combine/cptest/topicdefinition.txt</TT>.

<P>
</LI>
<LI>Load seed URLs
<BR><TT>combineCtrl  load <code>--</code>jobname cptest &lt; cpSeedURLs.txt</TT>

<P>
</LI>
<LI>Start 3 harvesting process
<BR><TT>combineCtrl  start <code>--</code>jobname cptest <code>--</code>harvesters 3</TT>

<P>
</LI>
<LI>Regularly export all data records in the ALVIS XML format
<BR><TT>combineExport <code>--</code>jobname cptest <code>--</code>profile alvis</TT>
<BR>
and schedule all pages for re-harvesting
<BR><TT>combineCtrl reharvest <code>--</code>jobname cptest</TT>

<P>
</LI>
</OL>

<DIV CLASS="navigation"><HR>
<!--Navigation Panel-->
<A NAME="tex2html280"
  HREF="node5.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next" SRC="next.png"></A> 
<A NAME="tex2html276"
  HREF="node2.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up" SRC="up.png"></A> 
<A NAME="tex2html270"
  HREF="node3.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous" SRC="prev.png"></A> 
<A NAME="tex2html278"
  HREF="node1.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents" SRC="contents.png"></A>  
<BR>
<B> Next:</B> <A NAME="tex2html281"
  HREF="node5.html">Configuration</A>
<B> Up:</B> <A NAME="tex2html277"
  HREF="node2.html">Overview</A>
<B> Previous:</B> <A NAME="tex2html271"
  HREF="node3.html">Introduction</A>
 &nbsp; <B>  <A NAME="tex2html279"
  HREF="node1.html">Contents</A></B> </DIV>
<!--End of Navigation Panel-->
<ADDRESS>
root
2006-11-29
</ADDRESS>
</BODY>
</HTML>
