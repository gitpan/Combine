<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">

<!--Converted with LaTeX2HTML 2002-2-1 (1.70)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<HTML>
<HEAD>
<TITLE>Frequently asked questions</TITLE>
<META NAME="description" CONTENT="Frequently asked questions">
<META NAME="keywords" CONTENT="DocMain">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">

<META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=iso-8859-1">
<META NAME="Generator" CONTENT="LaTeX2HTML v2002-2-1">
<META HTTP-EQUIV="Content-Style-Type" CONTENT="text/css">

<LINK REL="STYLESHEET" HREF="DocMain.css">

<LINK REL="next" HREF="node13.html">
<LINK REL="previous" HREF="node11.html">
<LINK REL="up" HREF="node11.html">
<LINK REL="next" HREF="node13.html">
</HEAD>

<BODY >

<DIV CLASS="navigation"><!--Navigation Panel-->
<A NAME="tex2html514"
  HREF="node13.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next" SRC="next.png"></A> 
<A NAME="tex2html510"
  HREF="node11.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up" SRC="up.png"></A> 
<A NAME="tex2html504"
  HREF="node11.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous" SRC="prev.png"></A> 
<A NAME="tex2html512"
  HREF="node1.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents" SRC="contents.png"></A>  
<BR>
<B> Next:</B> <A NAME="tex2html515"
  HREF="node13.html">Configuration variables</A>
<B> Up:</B> <A NAME="tex2html511"
  HREF="node11.html">Gory details</A>
<B> Previous:</B> <A NAME="tex2html505"
  HREF="node11.html">Gory details</A>
 &nbsp; <B>  <A NAME="tex2html513"
  HREF="node1.html">Contents</A></B> 
<BR>
<BR></DIV>
<!--End of Navigation Panel-->

<H1><A NAME="SECTION04010000000000000000">
Frequently asked questions</A>
</H1>

<OL>
<LI>What does the message 'Wide character in subroutine entry ...' mean?

<P>
</LI>
<LI>What does the message 'Parsing of undecoded UTF-8 will give garbage when decoding entities ...' mean?

<P>
</LI>
<LI>I can't figure out how to restrict the crawler to pages below
'<TT>http://www.foo.com/bar/</TT>'?

<P>
Put an
appropriate regular expression in the &lt;allow&gt; section of the configuration
file. Appropriate means
a Perl regular expression, which means that you have to escape special
characters. Try with
<BR><code>URL http:\/\/www\.foo\.com\/bar\/</code>

<P>
</LI>
<LI>I have a simple configuration variable set, but Combine does not obey it?

<P>
Check that there are not 2 instances of the same simple configuartion
variable in the same configuration file. Unfortunately this will break
configuration loading.

<P>
</LI>
<LI>If there are multiple &lt;allow&gt; entries, must an
URL fit all or any of them?

<P>
A match to any of the entries will make that URL allowable for crawling.
You can use any mix of HOST: and URL entries

<P>
</LI>
<LI>It would also be nice to be able to crawl local files.

<P>
Presently the crawler only accepts http, https, and ftp as protocols.

<P>
</LI>
<LI>Crawling of a single host is VERY slow.
Is there some way for me to speed the crawler up?

<P>
Yes it's one of the built-in limitations to keep the crawler beeing 'nice'
It will only access a particular server once every 60 seconds by default. You can change the default by adjusting the following configuration variables, but pleas keep in mind that you increase the load on the server.
<BR>
WaitIntervalSchedulerGetJcf=2
<BR>
WaitIntervalHost = 5
<BR>
<P>
</LI>
<LI>How can I crawl a fixed number of link steps from a set of seed
pages?
(Is it for example possible to crawl only one single webpage?
Or one webpage and all local links on that webpage (and not
any further)?)

<P>
Initialize the database and load the seed pages. Turn of automatic
recycling of links  by setting the simple configuration variable
'AutoRecycleLinks' to 0.

<P>
Start crawling and stop when '<TT>combineCtrl -jobname XXX howmany</TT>'
equals 0.

<P>
Handle recycling manually using 'combineCtrl, with action 'recyclelinks'.
(Give the command <TT>combineCtrl -jobname XXX recyclelinks</TT>')

<P>
Iterate to the depth of your liking.

<P>
</LI>
<LI>I run combineINIT but the configuration directory is not
created?

<P>
You need to run combineINIT as root, due to file protection
permissions.

<P>
</LI>
<LI>Where are the logs?

<P>
They are stored in the SQL database &lt;jobname&gt; in the table <TT>log</TT>.

<P>
</LI>
<LI>What are the main differences between Std and PosCheck algorithms for automated subject classification?

<P>
</LI>
<LI>I don't understand what this means. Can you explain it to me ? Thank you !

<P>
<PRE>
40: sundew[^\s]*=CP.Drosera
40: tropical pitcher plant=CP.Nepenthes
</PRE>

<P>
It's part of the topic definition (term list) for the topic 'Carnivorous plants'.
It's well described in the documenentation, please see
section <A HREF="node6.html#topicdef">4.5.1</A>.
The strange characters are Perl regular expressions mostly used for truncation etc.

<P>
</LI>
<LI>I want to get all pages about "icecream" from "www.yahoo.com". And I don't have clear idea about how to write the topic
definition file. Can you show me an example?

<P>
So for getting all pages about 'icecream' from 'www.yahoo.com' you have to:

<OL>
<LI>write a topic definition file according to the format above, eg containing topic specific
   terms. The file is essential a list of terms relevant for the topic. Format of the file is
   "numeric_importance: term=TopicClass" e.g. "<TT>100: icecream=YahooIce</TT>"
   (Say you call your topic 'YahooIce'). A few terms might be:
<BR><PRE>
100: icecream=YahooIce
100: ice cone=YahooIce
</PRE>
  and so on stored in a file called say TopicYahooIce.txt

<P>
</LI>
<LI>Initialization
<BR><TT>sudo combineINIT -jobname cptest -topic TopicYahooIce.txt</TT>

<P>
</LI>
<LI>Edit the configuration to only allow crawling of www.yahoo.com
Change the &lt;allow&gt; part in /etc/combine/focustest/combine.cfg from

<P>
<PRE>
#use either URL or HOST: (obs ':') to match regular expressions to either the
#full URL or the HOST part of a URL.
&lt;allow&gt;
#Allow crawl of URLs or hostnames that matches these regular expressions
HOST: .*$
&lt;/allow&gt;
</PRE>

<P>
to

<P>
<PRE>
#use either URL or HOST: (obs ':') to match regular expressions to either the
#full URL or the HOST part of a URL.
&lt;allow&gt;
#Allow crawl of URLs or hostnames that matches these regular expressions
HOST: www\.yahoo\.com$
&lt;/allow&gt;
</PRE>

<P>
</LI>
<LI>Load some good seed URLs

<P>
</LI>
<LI>Start 1 harvesting process
</LI>
</OL>

<P>
</LI>
<LI>Why load some good seeds URLs and what the seeds URLs mean.

<P>
This is just a way of telling the crawler where to start.

<P>
</LI>
<LI>My problem is that the
installation there requires root access, which I cannot get.
Is there a
way of running Combine without requiring any root access?

<P>
The are three things that are problematic

<OL>
<LI>Configurations are stored in /etc/combine/...
</LI>
<LI>Runtime PID files are stored in /var/run/combine
</LI>
<LI>You have to be able to create MySQL databases accessible by combine
Apart from that nothing else needs root access.
</LI>
</OL>

<P>
If you take the source and look how the tests (make test) are made you might find a way to
fix 1. Though this probably involves modifying the source - maybe only the Combine/Config.pm

<P>
2. is strictly not necessary and it will run even if /var/run /combine does not exist, although not
   the command 'combineCtrl -jobname XXX kill'

<P>
3. is necessary and I can't think of a way around it except making a local installation of MySQL and use
that.

<P>
</LI>
<LI>What does the following entries from the log table mean?

<OL>
<LI><code>| 5409 | HARVPARS 1_zltest | 2006-07-14 15:08:52 | M500; SD empty, sleep 20 second... |</code>

<P>
This means that there are no URLs ready for crawling (SD empty).
Also you can use combineCtrl to see current status of ready queue etc

<P>
</LI>
<LI><code>| 7352 | HARVPARS 1_wctest | 2006-07-14 17:00:59 | M500; urlid=1; netlocid=1; http://www.shanghaidaily.com/</code>

<P>
Crawler process 7352 got a url (http://www.shanghaidaily.com/) to check
(1_wctest is a just a name non significant)
M500 is a sequence number for an individual crawler starting at 500 and when it reaches 0 this crawler
process is killed and another is created.
urlid and netlocid are internal identifiers used in the MySQL tables.

<P>
</LI>
<LI><code>| 7352 | HARVPARS 1_wctest | 2006-07-14 17:01:10 | M500; RobotRules OK, OK</code>

<P>
Crawler process have checked that this url (identified earlier in the log by pid=7352 and M500) can be crawled according to the Robot Exclusin protocol.

<P>
</LI>
<LI><code>| 7352 | HARVPARS 1_wctest | 2006-07-14 17:01:10 | M500; HTTP(200 = "OK")  =&gt; OK</code>

<P>
It has fetched the page (identified earlier in the log by pid=7352 and M500) OK

<P>
</LI>
<LI><code>| 7352 | HARVPARS 1_wctest | 2006-07-14 17:01:10 | M500; Doing: text/html;200;0F061033DAF69587170F8E285E950120;Not used |</code>

<P>
It is processing the page (in the format text/html) to see if it is of topical interest
0F061033DAF69587170F8E285E950120 is the MD5 checksum of the page
</LI>
</OL>

<P>
</LI>
<LI>In fact , I  want to  know  which  crawed  urls  are  corresponding  to      the  certain  topic  class
such as CP.Aldrovanda . Can you tell me how can I know ?

<P>
You have to get into the raw MySQL database and perform a query like

<P>
SELECT urls.urlstr FROM urls,recordurl,topic WHERE urls.urlid=recordurl.urlid AND
recordurl.recordid=topic.recordid AND topic.notation='CP.Aldrovanda';

<P>
Table urls contain all URLs seen.
Table recordurl connect urlid to recordid.
recordid is used in all tables with data from the crawled Web pages.

<P>
</LI>
<LI>what is the meaning of the item "ALL"  in the notation column of the topic table ?

<P>
If you use multiple topics in your topic-defintion
(ie the string after '=') then all the relevant topic scores for this page is summed
and given the topic notation 'ALL'.

<P>
Just disregard it if you only use one topic-class.

<P>
</LI>
<LI>Combine should crawl all pages underneath, but not go outside
the domain (i.e. going to www.yahoo.com) but also not
going higher in position (i.e.
www.geocities.com/boulevar/atlanta/index.html).

<P>
Is it possible to set up Combine like this?

<P>
Yes, change the &lt;allow&gt;-part of your configuration file combine.cfg
to select what URL's should be allowed for crawling (by default everything
is allowed). See also section <A HREF="node6.html#urlfilt">4.3</A>.

<P>
So change
<BR><PRE>
&lt;allow&gt;
#Allow crawl of URLs or hostnames that matches these regular expressions
HOST: .*$
&lt;/allow&gt;
</PRE>
to something like

<P>
<PRE>
&lt;allow&gt;
#Allow crawl of URLs or hostnames that matches these regular expressions
URL http:\/\/www\.geocities\.com\/boulevard\/newyork\/
&lt;/allow&gt;
</PRE>

<P>
(the backslashes are needed since these patterns are in fact Perl regular expressions)

<P>
</LI>
</OL>

<P>

<DIV CLASS="navigation"><HR>
<!--Navigation Panel-->
<A NAME="tex2html514"
  HREF="node13.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next" SRC="next.png"></A> 
<A NAME="tex2html510"
  HREF="node11.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up" SRC="up.png"></A> 
<A NAME="tex2html504"
  HREF="node11.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous" SRC="prev.png"></A> 
<A NAME="tex2html512"
  HREF="node1.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents" SRC="contents.png"></A>  
<BR>
<B> Next:</B> <A NAME="tex2html515"
  HREF="node13.html">Configuration variables</A>
<B> Up:</B> <A NAME="tex2html511"
  HREF="node11.html">Gory details</A>
<B> Previous:</B> <A NAME="tex2html505"
  HREF="node11.html">Gory details</A>
 &nbsp; <B>  <A NAME="tex2html513"
  HREF="node1.html">Contents</A></B> </DIV>
<!--End of Navigation Panel-->
<ADDRESS>
root
2006-11-29
</ADDRESS>
</BODY>
</HTML>
